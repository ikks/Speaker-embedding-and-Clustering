{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":482590,"sourceType":"datasetVersion","datasetId":224795},{"sourceId":12405566,"sourceType":"datasetVersion","datasetId":7823336}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Voice clustering\n\nThis notebook makes a partition of a bunch of audios, each one from a single speaker and identifies sets containing audios from the same speaker. Using the [wespeaker toolkit](https://github.com/wenet-e2e/wespeaker).  This is called voice clustering.\n\nYou can find the [latest version](https://github.com/ikks/Speaker-embedding-and-Clustering/blob/master/auto/notebook/voiceclustering.ipynb) of this notebook. And the [notebook in Kaggle](https://www.kaggle.com/code/igortamara/voice-clustering)\n\nThe output is a zip file that holds the clustering information with a csv and a sqlite database.\n\n## üìú Instructions\n\nYou will need the audios; in this case, we are using the [carlfm dataset](https://www.kaggle.com/datasets/carlfm01/120h-spanish-speech).  Feel free to copy this notebook and adapt with your data.\n\nYou will also need a csv file that holds the transcriptions.\n\n## üëÄ Under the hood\n\nThe process is:\n\n* For each audio file create an embedding vector\n* Use HDBSCAN for the clustering\n* mark each audio with the id given by the clustering process\n\n[The git repo](https://github.com/ikks/Speaker-embedding-and-Clustering) contains all the details, including storing in sqlite, embedding and using wespeaker to calculate the embeddings.\n\n# üß∞ Software and Packages installation\n\nRun the next cell","metadata":{}},{"cell_type":"code","source":"%%bash\napt install sqlite3\ngit clone https://github.com/ikks/Speaker-embedding-and-Clustering.git clustering\npip install git+https://github.com/wenet-e2e/wespeaker.git\npip install scikit-learn==1.7.0 sqlite-vec tqdm\nmkdir -p /kaggle/working/output /kaggle/working/bin\n\necho -e \"\ncp /kaggle/working/db.sqlite /kaggle/working/output\nsqlite3 -quote -header /kaggle/working/output/db.sqlite 'SELECT * FROM files' > /kaggle/working/output/clustering.csv\ncd /kaggle/working\nzip -q dbdata.zip output/*\nmv dbdata.zip output/\nls -sh /kaggle/working/output\nrm output/clustering.csv\nrm output/db.sqlite\n\" > /kaggle/working/bin/updateversion.sh\nchmod +x  /kaggle/working/bin/updateversion.sh\n\necho -e \"\n    We are ready for the next step\n\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T18:13:30.533013Z","iopub.execute_input":"2025-07-11T18:13:30.533383Z","iopub.status.idle":"2025-07-11T18:13:30.547897Z","shell.execute_reply.started":"2025-07-11T18:13:30.533353Z","shell.execute_reply":"2025-07-11T18:13:30.546881Z"}},"outputs":[{"name":"stdout","text":"\n    We are ready for the next step\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# ‚öôÔ∏è Setup and task execution\n\nSet `csv_file` with the path of the csv file that contains three columns, the first one is the name of the wav file, and the third is the transcription of the file.\n\nSet`path_wavs` to the path of the directory that contains the wavs to be processed.\n\nRun the next cell once the required variable are setup.\n\n## ü™ì Stop and rerun the process\nYou can interrupt the task with  *Cancel Run* and continue were it left off running again the cell.","metadata":{}},{"cell_type":"code","source":"# Configure these two variables\ncsv_file = \"/kaggle/input/120h-spanish-speech/asr-spanish-v1-carlfm01/files.csv\"\npath_wavs = \"/kaggle/input/120h-spanish-speech/asr-spanish-v1-carlfm01/audios\"\n\n%cd /kaggle/working/clustering\nfrom speaker_embedding import nb_cluster_task\ndb_file = \"/kaggle/working/db.sqlite\"\nnb_cluster_task(db_file, csv_file, path_wavs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T18:23:50.568911Z","iopub.execute_input":"2025-07-11T18:23:50.569441Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/clustering\nusing device: cpu\nInitializing embedding model...\nEmbedding model initialized\nReviewing 112845 files\nLoading previously calculated embeddings...\nEmbeddings to be calculated: 103327\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/103327 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9104538afc4e487092a265dd8d9940e1"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"# üíæ Storing information\n\nWhen you run the next cell, you'll get /kaggle/working/output/dbdata.zip containing the sqlite and csv files.","metadata":{}},{"cell_type":"code","source":"%%bash\n/kaggle/working/bin/updateversion.sh\n\necho -e \"\n\nIn the right panel go to Output and download dbdata.zip  --->\n\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T18:22:56.790702Z","iopub.execute_input":"2025-07-11T18:22:56.791114Z","iopub.status.idle":"2025-07-11T18:22:57.854435Z","shell.execute_reply.started":"2025-07-11T18:22:56.791067Z","shell.execute_reply":"2025-07-11T18:22:57.853522Z"}},"outputs":[{"name":"stdout","text":"total 44M\n1.3M clustering.csv\n 31M dbdata.zip\n 13M db.sqlite\n\n\nIn the right panel go to Output and download dbdata.zip  --->\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# Possible improvements\n\n* Use diarization to use audio files containing multiple speakers. Consider using [nemo](https://github.com/nvidia/nemo/), [wespeaker](https://github.com/wenet-e2e/wespeaker) or pyannotate. If you find another, feel free to add a comment.\n* Use xi-vectors to update the cluster information.\n* Export to a dataset the results to download the data from Kaggle.  For now,[scp can be used](https://www.kaggle.com/code/igortamara/ssh-to-kaggle).\n# References\n\n* [Wespeaker](https://github.com/wenet-e2e/wespeaker) for identification, validation and speaker diarization.\n* [Blogpost](https://medium.com/@sapkotabinit2002/speaker-identification-and-clustering-using-pyannote-dbscan-and-cosine-similarity-dfa08b5b2a24) explaining concepts and from which this work took lot of information and code.\n* [Public Domain Spanish dataset](https://www.kaggle.com/datasets/carlfm01/120h-spanish-speech) from librivox ","metadata":{}}]}